{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f174ba-aec2-437d-802c-f0a05aa7efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import clip\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610d2c2-eb2c-4aff-a47f-d3674f0ecd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'gtsrb' #choose from ['imagenet', 'food101', 'gtsrb', 'places365']\n",
    "base_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f52035-d290-4605-be5a-ff987d8705f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n",
    "\n",
    "if dataset == 'imagenet':\n",
    "    from data.imagenet_labels import imagenet_classes as class_list\n",
    "    data_dir = f'{base_dir}/ImageNet_Validation/'\n",
    "    all_images = torchvision.datasets.ImageNet(data_dir, split='val', transform=clip_preprocess)\n",
    "\n",
    "elif dataset == 'food101':\n",
    "    data_dir = f'{base_dir}/Food101/'\n",
    "    all_images = torchvision.datasets.Food101(data_dir, split = 'test', transform = clip_preprocess)\n",
    "    \n",
    "    class_list = all_images.classes\n",
    "    class_list = [c.replace('_', ' ') for c in class_list]\n",
    "\n",
    "elif dataset == 'places365':\n",
    "    from data.places365_labels import places_classlist as class_list\n",
    "\n",
    "    data_dir = f'{base_dir}/Places365/'\n",
    "    all_images = torchvision.datasets.Places365(data_dir, split='val', transform = clip_preprocess)\n",
    "    \n",
    "elif dataset == 'gtsrb':\n",
    "    from data.gtsrb_labels import gtsrb_classes as class_list\n",
    "    \n",
    "    data_dir = f'{base_dir}/GTSRB/'\n",
    "    all_images = torchvision.datasets.GTSRB(data_dir, split = 'test', transform = clip_preprocess)\n",
    "else:\n",
    "    print('DATASET NOT IMPLEMENTED')\n",
    "\n",
    "token_text = [f'a photo of a {x}.' for x in class_list]\n",
    "num_base_classes = len(class_list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    clip_text = clip.tokenize(token_text).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc890da8-b831-405b-b54c-b899ce66f97c",
   "metadata": {},
   "source": [
    "## Standard test (no negative embeddings added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1c4ae-d41c-441e-9465-e974790e4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retest = True\n",
    "\n",
    "if not os.path.isfile(f'pred_files/clip/{dataset}/standard_gt_clip.npy') or not os.path.isfile(f'pred_files/clip/{dataset}/standard_cosine_clip.npy') or retest:\n",
    "    loader = torch.utils.data.DataLoader(all_images, batch_size=64, num_workers=4)\n",
    "    \n",
    "    all_cosine = []\n",
    "    gt_labels = []\n",
    "\n",
    "    for i, (images, target) in enumerate(tqdm(loader)):\n",
    "        image = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits_per_image, logits_per_text = clip_model(image, clip_text)\n",
    "    \n",
    "        all_cosine += logits_per_image.cpu().tolist()\n",
    "        gt_labels += target.cpu().tolist()\n",
    "\n",
    "    gt_labels = np.array(gt_labels)\n",
    "    all_cosine = np.array(all_cosine)\n",
    "    \n",
    "    with open(f'pred_files/clip/{dataset}/standard_gt_clip.npy', 'wb') as f:\n",
    "        np.save(f, gt_labels)\n",
    "    \n",
    "    with open(f'pred_files/clip/{dataset}/standard_cosine_clip.npy', 'wb') as f:\n",
    "        np.save(f, all_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446f05f-b6ef-4ac5-823f-d99cc0118e63",
   "metadata": {},
   "source": [
    "## Testing Negative Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a48f1-fc20-40ff-acd3-5c2b11dc1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retest = True\n",
    "\n",
    "for neg_count in [10, 50, 100, 250, 1000, 2500]:\n",
    "    for type in ['embedding', 'word']:\n",
    "        other_type = f'{neg_count}_{type}'\n",
    "        if not os.path.isfile(f'pred_files/clip/{dataset}/{other_type}_gt_clip.npy') or not os.path.isfile(f'pred_files/clip/{dataset}/{other_type}_cosine_clip.npy') or retest\n",
    "            loader = torch.utils.data.DataLoader(images, batch_size=64, num_workers=4)\n",
    "\n",
    "            extra_words = []\n",
    "            if 'word' in other_type:\n",
    "                count = int(other_type.replace('_word', ''))\n",
    "                all_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "                lengths = [2, 3, 4, 5, 6, 7, 8]\n",
    "                l_p = [0.01, 0.04, 0.2, 0.25, 0.25, 0.2]\n",
    "                \n",
    "                extra_words = []\n",
    "                \n",
    "                for i in range(neg_count):\n",
    "                    np.random.seed(i)\n",
    "                \n",
    "                    num = np.random.choice(lengths, 1, l_p)\n",
    "                    letters = np.random.choice(all_letters, num)\n",
    "                    word = ''.join(letters)\n",
    "                    extra_words += [f'a photo of a {word}.']\n",
    "                    \n",
    "            with torch.no_grad():\n",
    "                clip_text_other = clip.tokenize(token_text+extra_words).to(device)\n",
    "        \n",
    "            if 'emb' in other_type:\n",
    "                text_dims = clip_text_other.size(1)\n",
    "               \n",
    "                torch.manual_seed(0)\n",
    "                other_emb = torch.randint(49407, (neg_count, text_dims), dtype = torch.int32).cuda()\n",
    "                clip_text_other = torch.cat((clip_text_other, other_emb))\n",
    "\n",
    "\n",
    "            all_cosine = []\n",
    "            gt_labels = []\n",
    " \n",
    "            for i, (images, target) in enumerate(tqdm(loader)):\n",
    "                image = images.cuda()\n",
    "                target = target.cuda()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits_per_image, logits_per_text = clip_model(image, clip_text_other)\n",
    "            \n",
    "                all_cosine += logits_per_image.cpu().tolist()\n",
    "                gt_labels += target.cpu().tolist()\n",
    "        \n",
    "           \n",
    "            gt_labels = np.array(gt_labels)\n",
    "            all_cosine = np.array(all_cosine)\n",
    "            \n",
    "            with open(f'pred_files/clip/{dataset}/{other_type}_gt_clip.npy', 'wb') as f:\n",
    "                np.save(f, gt_labels)\n",
    "            \n",
    "            with open(f'pred_files/clip/{dataset}/{other_type}_cosine_clip.npy', 'wb') as f:\n",
    "                np.save(f, all_cosine)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
