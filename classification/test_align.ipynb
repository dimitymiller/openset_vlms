{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f174ba-aec2-437d-802c-f0a05aa7efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AlignProcessor, AlignModel\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610d2c2-eb2c-4aff-a47f-d3674f0ecd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'places365' #choose from ['imagenet', 'food101', 'gtsrb', 'places365']\n",
    "base_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f52035-d290-4605-be5a-ff987d8705f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_processor = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
    "align_model = AlignModel.from_pretrained(\"kakaobrain/align-base\").to(device)\n",
    "align_model = align_model.eval()\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "if dataset == 'imagenet':\n",
    "    from data.imagenet_labels import imagenet_classes as class_list\n",
    "    data_dir = f'{base_dir}/ImageNet_Validation/'\n",
    "    all_images = torchvision.datasets.ImageNet(data_dir, split='val')\n",
    "\n",
    "elif dataset == 'food101':\n",
    "    data_dir = f'{base_dir}/Food101/'\n",
    "    all_images = torchvision.datasets.Food101(data_dir, split = 'test')\n",
    "    \n",
    "    class_list = all_images.classes\n",
    "    class_list = [c.replace('_', ' ') for c in class_list]\n",
    "\n",
    "elif dataset == 'places365':\n",
    "    from data.places365_labels import places_classlist as class_list\n",
    "\n",
    "    data_dir = f'{base_dir}/Places365/'\n",
    "    all_images = torchvision.datasets.Places365(data_dir, split='val')\n",
    "    \n",
    "elif dataset == 'gtsrb':\n",
    "    from data.gtsrb_labels import gtsrb_classes as class_list\n",
    "    \n",
    "    data_dir = f'{base_dir}/GTSRB/'\n",
    "    all_images = torchvision.datasets.GTSRB(data_dir, split = 'test')\n",
    "else:\n",
    "    print('DATASET NOT IMPLEMENTED')\n",
    "\n",
    "token_text = [f'a photo of a {x}.' for x in class_list]\n",
    "num_base_classes = len(class_list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    align_text = align_processor(text = token_text, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc890da8-b831-405b-b54c-b899ce66f97c",
   "metadata": {},
   "source": [
    "## Standard test (no negative embeddings added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1c4ae-d41c-441e-9465-e974790e4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retest = True\n",
    "batch_size = 125\n",
    "if dataset == 'gtsrb':\n",
    "    batch_size = 30\n",
    "if not os.path.isfile(f'pred_files/align/{dataset}/standard_gt_align.npy') or not os.path.isfile(f'pred_files/align/{dataset}/standard_cosine_align.npy') or retest:\n",
    "    all_cosine = []\n",
    "    gt_labels = []\n",
    "\n",
    "    for i in tqdm(range(0, len(all_images), batch_size)):\n",
    "        ims = []\n",
    "        target = []\n",
    "        for j in range(batch_size):\n",
    "            im, t = all_images[i+j]\n",
    "            ims += [im]\n",
    "            target += [t]          \n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = align_processor(images=ims, return_tensors=\"pt\")\n",
    "        \n",
    "            inputs['pixel_values'] = inputs['pixel_values'].to(device)\n",
    "            for k in align_text.keys():\n",
    "                inputs[k] = align_text[k].to(device)\n",
    "                \n",
    "            outputs = align_model(**inputs)\n",
    "\n",
    "            logits_per_text = torch.matmul(outputs.text_embeds, outputs.image_embeds.t()) / align_model.temperature\n",
    "            logits_per_image = logits_per_text.t()\n",
    "            del logits_per_text, outputs\n",
    "            \n",
    "            all_cosine += logits_per_image.cpu().tolist()\n",
    "            gt_labels += target\n",
    "\n",
    "    gt_labels = np.array(gt_labels)\n",
    "    all_cosine = np.array(all_cosine)\n",
    "    \n",
    "    with open(f'pred_files/align/{dataset}/standard_gt_align.npy', 'wb') as f:\n",
    "        np.save(f, gt_labels)\n",
    "    \n",
    "    with open(f'pred_files/align/{dataset}/standard_cosine_align.npy', 'wb') as f:\n",
    "        np.save(f, all_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446f05f-b6ef-4ac5-823f-d99cc0118e63",
   "metadata": {},
   "source": [
    "## Testing Negative Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a48f1-fc20-40ff-acd3-5c2b11dc1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retest = True\n",
    "\n",
    "batch_size = 125\n",
    "if dataset == 'gtsrb':\n",
    "    batch_size = 30\n",
    "\n",
    "for neg_count in [10, 50, 100, 250, 1000, 2500]:\n",
    "    for type in ['embedding', 'word']:\n",
    "        other_type = f'{neg_count}_{type}'\n",
    "        if not os.path.isfile(f'pred_files/align/{dataset}/{other_type}_gt_align.npy') or not os.path.isfile(f'pred_files/align/{dataset}/{other_type}_cosine_align.npy') or retest\n",
    "            loader = torch.utils.data.DataLoader(images, batch_size=64, num_workers=4)\n",
    "\n",
    "            extra_words = []\n",
    "            if 'word' in other_type:\n",
    "                count = int(other_type.replace('_word', ''))\n",
    "                all_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "                lengths = [2, 3, 4, 5, 6, 7, 8]\n",
    "                l_p = [0.01, 0.04, 0.2, 0.25, 0.25, 0.2]\n",
    "                \n",
    "                extra_words = []\n",
    "                \n",
    "                for i in range(neg_count):\n",
    "                    np.random.seed(i)\n",
    "                \n",
    "                    num = np.random.choice(lengths, 1, l_p)\n",
    "                    letters = np.random.choice(all_letters, num)\n",
    "                    word = ''.join(letters)\n",
    "                    extra_words += [f'a photo of a {word}.']\n",
    "                    \n",
    "            with torch.no_grad():\n",
    "                align_text_other = align_processor(text = token_text+extra_words, return_tensors = \"pt\")\n",
    "    \n",
    "            if 'emb' in other_type:\n",
    "                text_dims = 640\n",
    "               \n",
    "                torch.manual_seed(0)\n",
    "                count = int(other_type.replace('_emb', ''))\n",
    "                other_emb = torch.Tensor(np.random.normal(0, 0.25, (count, text_dims)))\n",
    "                other_emb = (other_emb / other_emb.norm(p=2, dim=-1, keepdim=True)).cuda()\n",
    "            else:\n",
    "                other_emb = torch.Tensor([[]]).cuda()\n",
    "            \n",
    "            all_cosine = []\n",
    "            gt_labels = []\n",
    "            for i in tqdm(range(0, len(all_images), batch_size)):\n",
    "                ims = []\n",
    "                target = []\n",
    "                for j in range(batch_size):\n",
    "                    im, t = all_images[i+j]\n",
    "                    ims += [im]\n",
    "                    target += [t]          \n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    inputs = align_processor(images=ims, return_tensors=\"pt\")\n",
    "                \n",
    "                    inputs['pixel_values'] = inputs['pixel_values'].to(device)\n",
    "                    for k in align_text_other.keys():\n",
    "                        inputs[k] = align_text_other[k].to(device)\n",
    "                        \n",
    "                    outputs = align_model(**inputs)\n",
    "                    \n",
    "                    if 'emb' in other_type:\n",
    "                        text_embeds = torch.concat((outputs.text_embeds, other_emb))\n",
    "                    else:\n",
    "                        text_embeds = outputs.text_embeds\n",
    "            \n",
    "                    logits_per_text = torch.matmul(text_embeds, outputs.image_embeds.t()) / align_model.temperature\n",
    "                \n",
    "                logits_per_image = logits_per_text.t()\n",
    "                del logits_per_text, outputs, text_embeds\n",
    "                all_cosine += logits_per_image.cpu().tolist()\n",
    "                gt_labels += target\n",
    "            \n",
    "            gt_labels = np.array(gt_labels)\n",
    "            all_cosine = np.array(all_cosine)\n",
    "            \n",
    "            with open(f'pred_files/align/{dataset}/{other_type}_gt_align.npy', 'wb') as f:\n",
    "                np.save(f, gt_labels)\n",
    "            \n",
    "            with open(f'pred_files/align/{dataset}/{other_type}_cosine_align.npy', 'wb') as f:\n",
    "                np.save(f, all_cosine)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
